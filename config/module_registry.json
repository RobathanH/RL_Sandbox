{
  "EnvHandler": "env_handler.env_handler",
  "EnvHandler_Config": "env_handler.env_handler",
  "ExpMC": "exp_buffer.exp_format",
  "ExpSars": "exp_buffer.exp_format",
  "ExpSarsa": "exp_buffer.exp_format",
  "ExpTD": "exp_buffer.exp_format",
  "Trajectory": "exp_buffer.exp_format",
  "ExpBuffer": "exp_buffer.exp_buffer",
  "ExpBuffer_Config": "exp_buffer.exp_buffer",
  "Policy": "policy.policy",
  "Trainer": "trainer.trainer",
  "Trainer_Config": "trainer.trainer",
  "Config": "config.config",
  "ActionTransform": "env_transform.action_transform",
  "ObservationTransform": "env_transform.observation_transform",
  "ContinuousActionSpace": "env_handler.env_format",
  "DiscreteActionSpace": "env_handler.env_format",
  "SinglePlayerEnvHandler": "env_handler.singleplayer_env_handler",
  "SinglePlayerEnvHandler_Config": "env_handler.singleplayer_env_handler",
  "TDExpBuffer": "exp_buffer.td_exp_buffer",
  "TDExpBuffer_Config": "exp_buffer.td_exp_buffer",
  "FunctionApproximator": "function_approximator.function_approximator",
  "EpsilonGreedyFunctionPolicy": "policy.function_policy",
  "FunctionPolicy": "policy.function_policy",
  "Constant": "util.schedule",
  "LinearSchedule": "util.schedule",
  "LogarithmicSchedule": "util.schedule",
  "Schedule": "util.schedule",
  "Huber_Loss": "trainer.loss_functions",
  "Loss": "trainer.loss_functions",
  "MSE_Loss": "trainer.loss_functions",
  "ActionType": "trainer.actor_critic",
  "ActorCritic": "trainer.actor_critic",
  "ActorCritic_Config": "trainer.actor_critic",
  "Activation": "function_approximator.activation",
  "BoundedModule": "function_approximator.util",
  "ModuleTuple": "function_approximator.util",
  "ParameterModule": "function_approximator.util",
  "Linear": "function_approximator.basic_networks",
  "MLP": "function_approximator.basic_networks",
  "MultiheadModule": "function_approximator.basic_networks",
  "Parameter": "function_approximator.basic_networks",
  "SequentialModule": "function_approximator.basic_networks",
  "ABC": "abc",
  "Enum": "enum",
  "ASGD": "torch.optim.asgd",
  "Adadelta": "torch.optim.adadelta",
  "Adagrad": "torch.optim.adagrad",
  "Adam": "torch.optim.adam",
  "AdamW": "torch.optim.adamw",
  "Adamax": "torch.optim.adamax",
  "LBFGS": "torch.optim.lbfgs",
  "NAdam": "torch.optim.nadam",
  "Optimizer": "torch.optim.optimizer",
  "RAdam": "torch.optim.radam",
  "RMSprop": "torch.optim.rmsprop",
  "Rprop": "torch.optim.rprop",
  "SGD": "torch.optim.sgd",
  "SparseAdam": "torch.optim.sparse_adam",
  "QLearning": "trainer.q_learning",
  "QLearning_Config": "trainer.q_learning",
  "DoubleQLearning": "trainer.double_q_learning",
  "DoubleQLearning_Config": "trainer.double_q_learning",
  "VideoFileClip": "moviepy.video.io.VideoFileClip",
  "ConfigDecoder": "config.config_io",
  "ConfigEncoder": "config.config_io",
  "TemporaryDirectory": "tempfile",
  "ByteUnpacker": "env_transform.observation_byte_unpacker",
  "tqdm": "tqdm.std"
}
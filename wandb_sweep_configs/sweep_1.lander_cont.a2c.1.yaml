program: main.py
project: RL_Sandbox
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - "lander_cont.a2c.1"
  - "-t"
  - "500"
  - "--disable_recording"
  - "--overrides"
  - ${args_json}
method: bayes
metric:
  goal: maximize
  name: episode_reward
parameters:
  env_handler.time_limit_counts_as_terminal_state:
    values:
      - "true"
      - "false"
    distribution: categorical
  env_handler.discount_rate:
    max: 1
    min: 0.8
    distribution: uniform
  exp_buffer.td_steps:
    max: 10
    min: 1
    distribution: int_uniform
  trainer.soft_target_update_fraction:
    max: 1
    min: 1.0e-3
    distribution: log_uniform_values
  trainer.entropy_loss_constant:
    max: 1.0e-1
    min: 1.0e-6
    distribution: log_uniform_values
  trainer.weight_decay:
    max: 1.0e-1
    min: 1.0e-6
    distribution: log_uniform_values
  trainer.learning_rate.start:
    max: 1.0e-2
    min: 1.0e-6
    distribution: log_uniform_values
  trainer.learning_rate.end:
    max: 1.0e-2
    min: 1.0e-6
    distribution: log_uniform_values